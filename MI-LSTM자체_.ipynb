{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import data as d\n",
    "#import model\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas_datareader import data,wb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"testLine\\nkospi = StockData('StockChart/SAMPLE','StockChart/KOSPI.csv',20,50,10,10,0.7)\\nprint(np.shape(kospi.trainSet))\\nprint(np.shape(kospi.testSet))\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StockData():\n",
    "    \"\"\"\n",
    "        훈련용 데이터세트와 평가용 데이터세트를 만들기 위한 과정.\n",
    "\n",
    "        파라미터:\n",
    "            folderPath : 종목시장의 주가 csv가 저장되어있는 디렉토리 경로.\n",
    "            indexPath : 종목시장의 인덱스 주 csv파일의 경로.\n",
    "            timewindowsize : LSTM의 입력으로 들어가는 타임의 사이즈. 10일 경우 미래주가를 예측하기 위해 이전 10일 주가를 사용.\n",
    "            windowsizeForPCC : PCC를 계산하기 위해 사용되는 타임의 사이즈. 10일 경우 관계주를 구하기 위해 이전 10일 주가를 사용.\n",
    "            PositiveStockNumber : 긍정관계주 개수.\n",
    "            NegativeStockNumber : 부정관계주 개수.\n",
    "            train_test_rate : 훈련:평가 세트 비율. 0.7일 경우 생성된 데이터 세트중 70%는 훈련용, 30%는 평가용으로 사용.\n",
    "            batchSize : 세트를 나누는 배치 사이즈.\n",
    "\n",
    "        과정:\n",
    "            종목시장의 모든 종목의 시가와 인덱스 시가를 읽어옴.\n",
    "            minmax스케일러를 각 종목과 인덱스에 적용하고 저장.\n",
    "            PCC계산, 관계주 계산.\n",
    "            데이터세트 저장.\n",
    "               \n",
    "    \"\"\"\n",
    "    def __init__(self,folderPath,indexPath,timewindowsize,windowsizeForPCC,PostiveStockNumber,NegativeStockNumber,train_test_rate,batchSize):        \n",
    "               \n",
    "        if(train_test_rate<=0 or train_test_rate>=1):\n",
    "            raise ValueError('train_test_rate should be between 0 and 1')        \n",
    "        self.P=PostiveStockNumber\n",
    "        self.N=NegativeStockNumber        \n",
    "        self.T=timewindowsize\n",
    "        self.Tr=windowsizeForPCC\n",
    "        self.folderPath = folderPath      \n",
    "        self.indexPath=indexPath    \n",
    "        self.batchSize=batchSize\n",
    "        self.date_duration=40560\n",
    "        \n",
    "        self.train_test_rate=train_test_rate\n",
    "        self.scaler=MinMaxScaler(feature_range=(-1,1))  \n",
    "        self.indexScaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "        self.indexPrice = self.loadIndex()\n",
    "        self.stockPrice = self.loadCSV()\n",
    "\n",
    "        self.trainSet,self.testSet=self.make_dataset()\n",
    "\n",
    "        self.batchNum={}\n",
    "    \n",
    "    def getBatch(self,option):\n",
    "        \"\"\"\n",
    "        클래스에 저장된 세트를 y,xp,xn,xi,target으로 나누고 batch생성.\n",
    "\n",
    "        args:\n",
    "            option='training' or 'evaluation'\n",
    "\n",
    "        returns:\n",
    "            batch 제너레이터\n",
    "            batch={'y','xp','xn','xi','target'}\n",
    "        \"\"\"\n",
    "        if(option is not 'training' and option is not 'evaluation'):\n",
    "            raise ValueError('option should be \"training\" or \"evaluation\".')\n",
    "\n",
    "        if(option is 'training'):\n",
    "            returnSet = self.trainSet\n",
    "        else:\n",
    "            returnSet = self.testSet\n",
    "        \n",
    "        y=[]\n",
    "        xp=[]\n",
    "        xn=[]\n",
    "        xi=[]\n",
    "        target=[]\n",
    "\n",
    "        for d in returnSet:\n",
    "            y.append(d['target_history'])  \n",
    "            xp.append(d['pos_history'])       \n",
    "            xn.append(d['neg_history'])       \n",
    "            xi.append(d['index_history'])       \n",
    "            target.append(d['target_price'])             \n",
    "        y=np.reshape(y,(-1,self.T,1))\n",
    "        xp=np.reshape(xp,(-1,15,self.T,1))\n",
    "        xn=np.reshape(xn,(-1,15,self.T,1))\n",
    "        xi=np.reshape(xi,(-1,self.T,1))\n",
    "        target=np.reshape(target,(-1,1))\n",
    "\n",
    "        print(\"data1 : \" , y.shape,xp.shape,xn.shape,xi.shape,target.shape)     \n",
    "\n",
    "        batchNum=int(len(y)/self.batchSize)\n",
    "        self.batchNum[option]=batchNum\n",
    "\n",
    "        for i in range(batchNum):\n",
    "            yield {'y':y[i*self.batchSize:(i+1)*self.batchSize],\n",
    "                   'xp':xp[i*self.batchSize:(i+1)*self.batchSize],\n",
    "                   'xn':xn[i*self.batchSize:(i+1)*self.batchSize],\n",
    "                   'xi':xi[i*self.batchSize:(i+1)*self.batchSize],\n",
    "                   'target':target[i*self.batchSize:(i+1)*self.batchSize]}\n",
    "\n",
    "    def loadCSV(self):\n",
    "        \"\"\"\n",
    "        csv파일이 있는 폴더를 입력으로 받아 데이터를 읽어옴.\n",
    "        \"\"\"\n",
    "        csvList=os.listdir(self.folderPath)        \n",
    "        dataframe = pd.DataFrame([])\n",
    "        \n",
    "\n",
    "        for csv in csvList:\n",
    "            data=pd.read_csv(self.folderPath+'/'+csv,engine='python')\n",
    "            if(len(data)>=self.date_duration):\n",
    "                data=data[-self.date_duration-1:-1]\n",
    "                data=data.reset_index()\n",
    "                data=data['open']\n",
    "\n",
    "                dataframe=dataframe.append(data,ignore_index=True)\n",
    "        dataT=np.array(dataframe).T\n",
    "        self.scaler.fit(dataT)\n",
    "        dataT=self.scaler.transform(dataT)\n",
    "        dataT=dataT.T\n",
    "        dataframe=pd.DataFrame(dataT)\n",
    "        dataframe=dataframe.transpose()\n",
    "        print('StockPrice shape: ',dataframe.shape)\n",
    "        return dataframe\n",
    "    \n",
    "    def loadIndex(self):\n",
    "        data=pd.read_csv(self.indexPath,engine='python')\n",
    "        data=data[-self.date_duration-1:-1]        \n",
    "        data=data.reset_index()\n",
    "        data=data.fillna(method='ffill')\n",
    "\n",
    "        data=np.array(data['Open'])\n",
    "        data=np.reshape(data,(-1,1))\n",
    "\n",
    "        data=self.indexScaler.fit_transform(data)\n",
    "\n",
    "        data=pd.DataFrame(np.squeeze(data))        \n",
    "\n",
    "        print('IndexPrice shape: ',data.shape)\n",
    "        return data \n",
    "\n",
    "\n",
    "    def make_dataset(self):\n",
    "        \"\"\"\n",
    "        예측모델에 사용되는 입력,타겟 데이터세트.\n",
    "        입력데이터의 shape는 (목표주식+관계주식+인덱스, 타임윈도우사이즈)\n",
    "        타겟데이터의 shape는 (1,1)\n",
    "        \"\"\"\n",
    "        maxday=max([self.T,self.Tr])\n",
    "        dataset=[]\n",
    "\n",
    "        for i in range(maxday,len(self.stockPrice)):\n",
    "            print('making dataset progress : {}/{}'.format(i,len(self.stockPrice)),end='\\r')\n",
    "            priceSet=self.stockPrice.loc[i-self.T:i-1]\n",
    "            targetSet=self.stockPrice.loc[i]\n",
    "            positiveSet,negativeSet=self.calculate_correlation(self.stockPrice.loc[i-maxday:i-1])\n",
    "            indexSet = self.indexPrice.loc[i-self.T:i-1]\n",
    "\n",
    "            for targetNum in priceSet.columns:\n",
    "                target_history=np.reshape(np.array(priceSet[targetNum]),(self.T,1))\n",
    "                pos_history=np.reshape(np.array(positiveSet[targetNum].T),(15,self.T,1))\n",
    "                neg_history=np.reshape(np.array(negativeSet[targetNum].T),(15,self.T,1))\n",
    "                index_history=np.reshape(np.array(indexSet),(self.T,1))\n",
    "                target_price=np.reshape(np.array(targetSet[targetNum]),(1,1))\n",
    "\n",
    "                dataset.append({'target_history':target_history,\n",
    "                                'pos_history':pos_history,\n",
    "                                'neg_history':neg_history,\n",
    "                                'index_history':index_history,\n",
    "                                'target_price':target_price\n",
    "                            })\n",
    "        print('making dataset progress : finished\\t')\n",
    "        \n",
    "        return dataset[:int(len(dataset)*self.train_test_rate)],dataset[int(len(dataset)*self.train_test_rate):]\n",
    "\n",
    "    def calculate_correlation(self,priceSet):\n",
    "        \"\"\"\n",
    "        Pearson Correlation Coefficient(PCC)를 계산하고,\n",
    "        높은순으로 긍정관계주, 낮은순으로 부정관계주를 설정한 개수만큼 생성하여 리스트에 저장한후, 리턴.\n",
    "        입력은 전체 종목의 타임윈도우간의 주가.\n",
    "\n",
    "        Returns:\n",
    "            #모든종목의 관계주. \n",
    "            긍정관계주 shape = (종목 수, dataframe(T*P))\n",
    "            부정관계주 shape = (종목 수, dataframe(T*N))\n",
    "        \"\"\"    \n",
    "        positive=[]\n",
    "        negative=[] \n",
    "        corr=priceSet[-self.Tr:].corr(method='pearson')\n",
    "\n",
    "        for i in corr.columns:\n",
    "            tempCorr=corr[i].sort_values(ascending=False)\n",
    "            index_P=tempCorr[1:self.P+1].index\n",
    "            index_N=tempCorr[-self.N:].index\n",
    "            \n",
    "            priceSet=priceSet[-self.T:]\n",
    "            posSet=priceSet[index_P]\n",
    "            negSet=priceSet[index_N]\n",
    "            posSet.columns=range(self.P)\n",
    "            negSet.columns=range(self.N)\n",
    "            \n",
    "            positive.append(posSet)\n",
    "            negative.append(negSet)\n",
    "        return positive,negative\n",
    "\n",
    "            \n",
    "\"\"\"testLine\n",
    "kospi = StockData('StockChart/SAMPLE','StockChart/KOSPI.csv',20,50,10,10,0.7)\n",
    "print(np.shape(kospi.trainSet))\n",
    "print(np.shape(kospi.testSet))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn=tf.compat.v1.nn.rnn_cell\n",
    "class base_LSTMCell(rnn.BasicLSTMCell):\n",
    "    def __call__(self,inputs,state,scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = tf.split(1, 2, state)\n",
    "            concat = tf.layers.dense(tf.concat([inputs, h],axis=1), 4 * self._num_units)\n",
    "\n",
    "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i, j, f, o = tf.split(concat, 4, 1)\n",
    "\n",
    "            new_c = (c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) *\n",
    "                    self._activation(j))\n",
    "            new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = rnn.LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = tf.concat(1, [new_c, new_h])\n",
    "        return new_h, new_state\n",
    "\n",
    "class MI_LSTMCell(rnn.BasicLSTMCell):\n",
    "    \"\"\"\n",
    "    Multi-Input LSTM proposed in the paper, Stock Price Prediction Using Attention-based Multi-Input LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               num_units,\n",
    "               num_inputs,\n",
    "               forget_bias=1.0,\n",
    "               state_is_tuple=True,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               name=None,\n",
    "               dtype=None,\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the basic LSTM cell.\n",
    "        args:\n",
    "            num_inputs: MI-LSTM의 입력의 개수. \n",
    "                이 파라미터에 따라 입력 게이트의 어텐션 레이어를 설정.\n",
    "                최소 1개이상.\n",
    "                1개일 경우, 어텐션 레이어를 제외하고 기본 LSTM과 동일.\n",
    "        \"\"\"        \n",
    "        super(MI_LSTMCell,self).__init__(num_units,\n",
    "               forget_bias=1.0,\n",
    "               state_is_tuple=True,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               name=None,\n",
    "               dtype=None,\n",
    "               **kwargs)\n",
    "        \n",
    "        if(type(num_inputs) is not int):\n",
    "            raise ValueError(\"num_inputs should be integer\")\n",
    "        if(num_inputs < 1):\n",
    "            raise ValueError(\"num_inputs should not be less than 0\")\n",
    "        self.num_inputs = num_inputs\n",
    "        self.alpha_weight=self.add_variable('alpha_weight',shape=[self._num_units,self._num_units])\n",
    "        self.alpha_bias=[]\n",
    "        for i in range(self.num_inputs):\n",
    "            self.alpha_bias.append(self.add_variable('alpha_bias'+str(i),shape=[1],initializer=tf.zeros_initializer()))\n",
    "\n",
    "    def __call__(self,inputs,state,scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.compat.v1.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = tf.split(1, 2, state)\n",
    "            inputs_list = tf.split(inputs,self.num_inputs,1)\n",
    "            concat = tf.compat.v1.layers.dense(tf.concat([inputs_list[0], h],axis=1), (3+self.num_inputs) * self._num_units)\n",
    "                                 \n",
    "            # 0 = forget_gate, 1 = output_gate, 2= main_new_input, 3 = main_input_gate, 4~ = input_gate_for_auxiliary\n",
    "            main_list = tf.split(concat, 3+self.num_inputs, 1)\n",
    "                        \n",
    "            #new_input_gate= list of all new_input.\n",
    "            new_input_gate=[tf.tanh(main_list[2])]\n",
    "            #linear layer for auxiliary inputs.\n",
    "            for i in range(1,self.num_inputs):\n",
    "                new_input_gate.append(tf.compat.v1.layers.dense(tf.concat([inputs_list[i], h],axis=1),self._num_units,activation=tf.tanh))\n",
    "\n",
    "            #making list of l. l = sigmoid(input_gate) * tanh(new_input)\n",
    "            new_l=[]\n",
    "            for i,new_input in enumerate(new_input_gate,3):\n",
    "                new_l.append(tf.sigmoid(main_list[i]) * new_input)\n",
    "\n",
    "\n",
    "            #making list of u.            \n",
    "            u=[]\n",
    "            for i,l in enumerate(new_l):\n",
    "                #temp = transpos(l) X W X Cell_State.\n",
    "                temp1=tf.matmul(l,self.alpha_weight)\n",
    "                temp1=tf.expand_dims(temp1,1)\n",
    "                temp2=tf.matmul(temp1,tf.expand_dims(c,2))\n",
    "                u.append(tf.tanh(tf.squeeze(temp2+self.alpha_bias[i],axis=2)))\n",
    "\n",
    "            #making list of alpha.\n",
    "            alpha=tf.nn.softmax(u,axis=0)\n",
    "\n",
    "            #making L.\n",
    "            L=[]\n",
    "            for i,l in enumerate(new_l):\n",
    "                L.append(alpha[i]*l)\n",
    "            L=tf.reduce_sum(L,axis=0)\n",
    "\n",
    "\n",
    "            #new state = c(t-1) * f + L. new h = tanh(c) + sigmoid(o)\n",
    "            new_c = (c * tf.sigmoid(main_list[0] + self._forget_bias)+L)\n",
    "            new_h = self._activation(new_c) * tf.sigmoid(main_list[1])\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = rnn.LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = tf.concat(1, [new_c, new_h])\n",
    "        return new_h, new_state\n",
    "\n",
    "\n",
    "class Attention_Layer():\n",
    "    \"\"\"\n",
    "    어텐션 레이어.\n",
    "    (None, TimeWindow, hidden_unit_size) shape의 LSTM 출력을 입력으로 받아 (None, 1, hidden_unit_size)의 텐서 출력.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        timewindow_size,\n",
    "        input_hidden_unit_size,\n",
    "        attention_size=None):\n",
    "        \"\"\"\n",
    "        Setting parameter for attention layer.\n",
    "        args:\n",
    "            timewindow_size = time window size of previous lstm layer.\n",
    "            input_hidden_unit_size = hidden unit number of previous lstm layer.\n",
    "            attention_size = size of this attention. \n",
    "                default = input_hidden_unit_size.\n",
    "        \"\"\"\n",
    "        if(attention_size is None):\n",
    "            attention_size=input_hidden_unit_size\n",
    "        self.o_size=attention_size\n",
    "        self.h_size=input_hidden_unit_size\n",
    "        self.t_size=timewindow_size\n",
    "\n",
    "        self.beta_weight=tf.Variable(tf.random.normal([self.h_size,self.o_size]), name='beta_weight')\n",
    "        self.beta_bias=tf.Variable(tf.zeros([self.o_size]),name='beta_bias')\n",
    "\n",
    "        self.v=tf.Variable(tf.random.normal([self.o_size,1]),name='beta_v')\n",
    "\n",
    "    def __call__(self,inputs):\n",
    "        \"\"\"\n",
    "        producing output with actual inputs.\n",
    "        shape of output will be (batch_size, 1, input_hidden_unit_size).\n",
    "        \"\"\"\n",
    "        #temp = tanh(Y X W + b) ->shape of result = (-1, self.o_size)\n",
    "        temp=tf.matmul(tf.reshape(inputs,[-1,self.h_size]),self.beta_weight)\n",
    "         \n",
    "        temp=tf.tanh(temp+self.beta_bias)\n",
    "        \n",
    "            \n",
    "        #j=temp X v\n",
    "        j=tf.reshape(tf.matmul(temp,self.v),[-1,self.t_size,1])\n",
    "\n",
    "        beta=tf.nn.softmax(j)\n",
    "        \n",
    "        \n",
    "\n",
    "        output=beta*inputs\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \"\"\"\n",
    "    모든 예측모델들의 기본 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self,sess,name,windowsize,Pos,Neg):\n",
    "        self.sess=sess\n",
    "        self.name=name\n",
    "        self.T=windowsize\n",
    "        self.P=Pos\n",
    "        self.N=Neg\n",
    "\n",
    "        \n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        pass\n",
    "\n",
    "class LSTM_Model(Model):\n",
    "    \"\"\"\n",
    "    Basic LSTM list for test.\n",
    "    \"\"\"\n",
    "    def _build_net(self):\n",
    "        self.Y=tf.compat.v1.placeholder(tf.float32,[None,self.T,1])\n",
    "        self.Xp=tf.compat.v1.placeholder(tf.float32,[None,self.P,self.T,1])\n",
    "        self.Xn=tf.compat.v1.placeholder(tf.float32,[None,self.N,self.T,1])\n",
    "        self.Xi=tf.compat.v1.placeholder(tf.float32,[None,self.T,1])\n",
    "        self.Target=tf.compat.v1.placeholder(tf.float32,[None,1])\n",
    "\n",
    "\n",
    "        Xps=tf.split(self.Xp,self.P,1)\n",
    "        Xns=tf.split(self.Xn,self.N,1)\n",
    "        Xp_list=[]\n",
    "        Xn_list=[]\n",
    "\n",
    "    \n",
    "        LSTM=tf.compat.v1.nn.rnn_cell.LSTMCell(64,name='lstm1')\n",
    "        \n",
    "        Y_1,_=tf.compat.v1.nn.dynamic_rnn(LSTM,self.Y,dtype=tf.float32)\n",
    "        Xi_1,_=tf.compat.v1.nn.dynamic_rnn(LSTM,self.Xi,dtype=tf.float32)\n",
    "        for i in range(len(Xps)):\n",
    "            o,_=tf.compat.v1.nn.dynamic_rnn(LSTM,tf.squeeze(Xps[i],axis=1),dtype=tf.float32)\n",
    "            Xp_list.append(o)\n",
    "        for i in range(len(Xns)):\n",
    "            o,_=tf.compat.v1.nn.dynamic_rnn(LSTM,tf.squeeze(Xns[i],axis=1),dtype=tf.float32)\n",
    "            Xn_list.append(o)\n",
    "        Xp_1=tf.reduce_mean(Xp_list,0)\n",
    "        Xn_1=tf.reduce_mean(Xn_list,0)\n",
    "\n",
    "        result=tf.concat([Y_1,Xp_1,Xn_1,Xi_1],axis=2)\n",
    "       \n",
    "\n",
    "        #MI-LSTM\n",
    "        LSTM2=MI_LSTMCell(64,4,name='lstm2')\n",
    "       \n",
    "        Y_2,_ =tf.compat.v1.nn.dynamic_rnn(LSTM2,result,dtype=tf.float32)\n",
    "       \n",
    "\n",
    "        #Attention_Layer\n",
    "        attention_layer=Attention_Layer(self.T,64)\n",
    "        \n",
    "        Y_3=attention_layer(Y_2)\n",
    "        \n",
    "\n",
    "        #Non-linear units for producing final prediction.\n",
    "        R_1=tf.compat.v1.layers.dense(tf.compat.v1.layers.flatten(Y_3),64,tf.nn.relu)\n",
    "        \n",
    "        R_2=tf.compat.v1.layers.dense(R_1,64,tf.nn.relu)\n",
    "        \n",
    "        R_3=tf.compat.v1.layers.dense(R_2,64,tf.nn.relu)\n",
    "        \n",
    "        R_4=tf.compat.v1.layers.dense(R_3,64,tf.nn.relu)\n",
    "        \n",
    "        R_5=tf.compat.v1.layers.dense(R_4,64,tf.nn.relu)\n",
    "        \n",
    "        R_6=tf.compat.v1.layers.dense(R_5,1)\n",
    "       \n",
    "        self.out=R_6\n",
    "        \n",
    "        self.cost=tf.compat.v1.losses.mean_squared_error(labels=self.Target,predictions=self.out)\n",
    "        self.optimizer=tf.compat.v1.train.AdamOptimizer(0.001).minimize(self.cost)\n",
    "\n",
    "    def outputs(self,y,xp,xn,xi,target):\n",
    "        fd={self.Y:y,self.Xp:xp,self.Xn:xn,self.Xi:xi,self.Target:target}\n",
    "        return self.sess.run(self.out,feed_dict=fd)\n",
    "\n",
    "    def training(self,y,xp,xn,xi,target):\n",
    "        fd={self.Y:y,self.Xp:xp,self.Xn:xn,self.Xi:xi,self.Target:target}\n",
    "        return self.sess.run([self.cost,self.optimizer],feed_dict=fd)\n",
    "\n",
    "    def returnCost(self,y,xp,xn,xi,target):\n",
    "        fd={self.Y:y,self.Xp:xp,self.Xn:xn,self.Xi:xi,self.Target:target}\n",
    "        return self.sess.run(self.cost,feed_dict=fd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexPrice shape:  (40559, 1)\n",
      "StockPrice shape:  (40559, 81)\n",
      "making dataset progress : finished\t59\n",
      "\n",
      "#training#\n",
      "WARNING:tensorflow:From <ipython-input-5-17a179c18b7c>:36: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-17a179c18b7c>:38: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/nextgen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:966: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/nextgen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/nextgen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:970: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-4-924828131441>:55: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-924828131441>:76: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/nextgen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-17a179c18b7c>:65: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  11.391085441533505\n",
      "evalution_cost :  588.0763257532963\n",
      "epoch : 0, t_cost : 0.002541, e_cost : 0.306130, elapsed time : 1752.26sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  5.205778270472365\n",
      "evalution_cost :  336.17292011553945\n",
      "epoch : 1, t_cost : 0.001161, e_cost : 0.174999, elapsed time : 1731.76sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  2.549098514864454\n",
      "evalution_cost :  226.97018807561471\n",
      "epoch : 2, t_cost : 0.000569, e_cost : 0.118152, elapsed time : 1730.21sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  1.3987610578697058\n",
      "evalution_cost :  121.35345391860756\n",
      "epoch : 3, t_cost : 0.000312, e_cost : 0.063172, elapsed time : 1728.23sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.8848151588790643\n",
      "evalution_cost :  75.82613726540148\n",
      "epoch : 4, t_cost : 0.000197, e_cost : 0.039472, elapsed time : 1723.02sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.737028809991898\n",
      "evalution_cost :  65.23711181199906\n",
      "epoch : 5, t_cost : 0.000164, e_cost : 0.033960, elapsed time : 1720.08sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.7017590491450392\n",
      "evalution_cost :  60.40436710739232\n",
      "epoch : 6, t_cost : 0.000157, e_cost : 0.031444, elapsed time : 1724.99sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.6943773887860516\n",
      "evalution_cost :  59.60394623898901\n",
      "epoch : 7, t_cost : 0.000155, e_cost : 0.031028, elapsed time : 1726.16sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.6654403802103843\n",
      "evalution_cost :  60.91290416195261\n",
      "epoch : 8, t_cost : 0.000148, e_cost : 0.031709, elapsed time : 1728.64sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.6628653807865703\n",
      "evalution_cost :  59.0739741644029\n",
      "epoch : 9, t_cost : 0.000148, e_cost : 0.030752, elapsed time : 1725.35sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.6293157970440006\n",
      "evalution_cost :  55.44048584115262\n",
      "epoch : 10, t_cost : 0.000140, e_cost : 0.028860, elapsed time : 1727.19sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.5995402597654902\n",
      "evalution_cost :  52.82069934041829\n",
      "epoch : 11, t_cost : 0.000134, e_cost : 0.027496, elapsed time : 1727.94sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.589234829646557\n",
      "evalution_cost :  51.410251789422546\n",
      "epoch : 12, t_cost : 0.000131, e_cost : 0.026762, elapsed time : 1725.05sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.5794119528391093\n",
      "evalution_cost :  48.68197759470422\n",
      "epoch : 13, t_cost : 0.000129, e_cost : 0.025342, elapsed time : 1725.36sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.560391990467906\n",
      "evalution_cost :  46.53871342378807\n",
      "epoch : 14, t_cost : 0.000125, e_cost : 0.024226, elapsed time : 1722.32sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.5487971659231334\n",
      "evalution_cost :  44.750182953494004\n",
      "epoch : 15, t_cost : 0.000122, e_cost : 0.023295, elapsed time : 1725.51sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.5367233089373258\n",
      "evalution_cost :  44.2196961498812\n",
      "epoch : 16, t_cost : 0.000120, e_cost : 0.023019, elapsed time : 1726.73sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.5245157775070766\n",
      "evalution_cost :  44.00606236856402\n",
      "epoch : 17, t_cost : 0.000117, e_cost : 0.022908, elapsed time : 1751.20sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.5187768146388407\n",
      "evalution_cost :  42.91693318304715\n",
      "epoch : 18, t_cost : 0.000116, e_cost : 0.022341, elapsed time : 1726.34sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.5168875009376279\n",
      "evalution_cost :  40.347917431867245\n",
      "epoch : 19, t_cost : 0.000115, e_cost : 0.021004, elapsed time : 1724.41sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.5034756378890961\n",
      "evalution_cost :  39.41592616198068\n",
      "epoch : 20, t_cost : 0.000112, e_cost : 0.020518, elapsed time : 1725.63sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4961436558096466\n",
      "evalution_cost :  39.20841335802652\n",
      "epoch : 21, t_cost : 0.000111, e_cost : 0.020410, elapsed time : 1724.26sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4929746503385104\n",
      "evalution_cost :  36.271953842360745\n",
      "epoch : 22, t_cost : 0.000110, e_cost : 0.018882, elapsed time : 1730.87sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4828168707572331\n",
      "evalution_cost :  35.729089109056076\n",
      "epoch : 23, t_cost : 0.000108, e_cost : 0.018599, elapsed time : 1724.72sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4764675626884127\n",
      "evalution_cost :  35.13567326629345\n",
      "epoch : 24, t_cost : 0.000106, e_cost : 0.018290, elapsed time : 1719.47sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.470800690474789\n",
      "evalution_cost :  33.75771667651861\n",
      "epoch : 25, t_cost : 0.000105, e_cost : 0.017573, elapsed time : 1725.76sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4711563611517704\n",
      "evalution_cost :  33.86084802213554\n",
      "epoch : 26, t_cost : 0.000105, e_cost : 0.017627, elapsed time : 1724.38sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.46885359561656514\n",
      "evalution_cost :  33.38219447656593\n",
      "epoch : 27, t_cost : 0.000105, e_cost : 0.017378, elapsed time : 1724.96sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.46328726379670115\n",
      "evalution_cost :  31.479901667764352\n",
      "epoch : 28, t_cost : 0.000103, e_cost : 0.016387, elapsed time : 1723.77sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4600743501123361\n",
      "evalution_cost :  32.715596939198804\n",
      "epoch : 29, t_cost : 0.000103, e_cost : 0.017031, elapsed time : 1725.08sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4576531849670573\n",
      "evalution_cost :  32.49879086034889\n",
      "epoch : 30, t_cost : 0.000102, e_cost : 0.016918, elapsed time : 1723.93sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.45592376493459597\n",
      "evalution_cost :  31.980911630787887\n",
      "epoch : 31, t_cost : 0.000102, e_cost : 0.016648, elapsed time : 1724.53sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4473954768563999\n",
      "evalution_cost :  30.225275978538775\n",
      "epoch : 32, t_cost : 0.000100, e_cost : 0.015734, elapsed time : 1723.34sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.44609791403672716\n",
      "evalution_cost :  30.193780744280957\n",
      "epoch : 33, t_cost : 0.000100, e_cost : 0.015718, elapsed time : 1726.63sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.44223559799684153\n",
      "evalution_cost :  30.44303579658481\n",
      "epoch : 34, t_cost : 0.000099, e_cost : 0.015847, elapsed time : 1725.22sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4429731799991714\n",
      "evalution_cost :  31.43469780736814\n",
      "epoch : 35, t_cost : 0.000099, e_cost : 0.016364, elapsed time : 1728.11sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.43791144656825054\n",
      "evalution_cost :  29.747585234383223\n",
      "epoch : 36, t_cost : 0.000098, e_cost : 0.015485, elapsed time : 1728.04sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4320851090060387\n",
      "evalution_cost :  28.428172801019173\n",
      "epoch : 37, t_cost : 0.000096, e_cost : 0.014799, elapsed time : 1724.06sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.43095411346803303\n",
      "evalution_cost :  30.868513856623395\n",
      "epoch : 38, t_cost : 0.000096, e_cost : 0.016069, elapsed time : 1721.60sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4263991008456287\n",
      "evalution_cost :  29.140294688662834\n",
      "epoch : 39, t_cost : 0.000095, e_cost : 0.015169, elapsed time : 1735.98sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.42418943252778263\n",
      "evalution_cost :  29.694326584003647\n",
      "epoch : 40, t_cost : 0.000095, e_cost : 0.015458, elapsed time : 1722.83sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.42233177828165935\n",
      "evalution_cost :  29.23039497722857\n",
      "epoch : 41, t_cost : 0.000094, e_cost : 0.015216, elapsed time : 1676.16sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4204171353885613\n",
      "evalution_cost :  29.47496638168377\n",
      "epoch : 42, t_cost : 0.000094, e_cost : 0.015344, elapsed time : 1679.40sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.41889921450274414\n",
      "evalution_cost :  28.992974379892985\n",
      "epoch : 43, t_cost : 0.000093, e_cost : 0.015093, elapsed time : 1679.17sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.415286713579917\n",
      "evalution_cost :  29.616988020603458\n",
      "epoch : 44, t_cost : 0.000093, e_cost : 0.015417, elapsed time : 1673.04sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.41521994530739903\n",
      "evalution_cost :  28.576414792072683\n",
      "epoch : 45, t_cost : 0.000093, e_cost : 0.014876, elapsed time : 1674.30sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.41173463281666045\n",
      "evalution_cost :  29.306569970452983\n",
      "epoch : 46, t_cost : 0.000092, e_cost : 0.015256, elapsed time : 1676.78sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4126475810298871\n",
      "evalution_cost :  30.296583922898208\n",
      "epoch : 47, t_cost : 0.000092, e_cost : 0.015771, elapsed time : 1679.08sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4078377177747825\n",
      "evalution_cost :  29.087303092885122\n",
      "epoch : 48, t_cost : 0.000091, e_cost : 0.015142, elapsed time : 1677.51sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4046447867513052\n",
      "evalution_cost :  29.004458576293473\n",
      "epoch : 49, t_cost : 0.000090, e_cost : 0.015099, elapsed time : 1673.82sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4064556975972664\n",
      "evalution_cost :  29.55162207669673\n",
      "epoch : 50, t_cost : 0.000091, e_cost : 0.015383, elapsed time : 1670.56sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4061594475269885\n",
      "evalution_cost :  28.972135081476154\n",
      "epoch : 51, t_cost : 0.000091, e_cost : 0.015082, elapsed time : 1674.51sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4031419671537151\n",
      "evalution_cost :  29.51094235452183\n",
      "epoch : 52, t_cost : 0.000090, e_cost : 0.015362, elapsed time : 1672.07sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3999732435013357\n",
      "evalution_cost :  27.845034101053898\n",
      "epoch : 53, t_cost : 0.000089, e_cost : 0.014495, elapsed time : 1675.27sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.4006840534202638\n",
      "evalution_cost :  29.737831040994934\n",
      "epoch : 54, t_cost : 0.000089, e_cost : 0.015480, elapsed time : 1673.46sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3935464020132713\n",
      "evalution_cost :  27.7094389195845\n",
      "epoch : 55, t_cost : 0.000088, e_cost : 0.014424, elapsed time : 1675.28sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.395817561649892\n",
      "evalution_cost :  28.625784618598118\n",
      "epoch : 56, t_cost : 0.000088, e_cost : 0.014902, elapsed time : 1676.90sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3888421165438558\n",
      "evalution_cost :  30.028243590624697\n",
      "epoch : 57, t_cost : 0.000087, e_cost : 0.015632, elapsed time : 1672.92sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.38892479689457105\n",
      "evalution_cost :  30.317537938324676\n",
      "epoch : 58, t_cost : 0.000087, e_cost : 0.015782, elapsed time : 1674.01sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.39109736667705874\n",
      "evalution_cost :  30.605428522443617\n",
      "epoch : 59, t_cost : 0.000087, e_cost : 0.015932, elapsed time : 1668.45sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3910187371438951\n",
      "evalution_cost :  30.15695616798257\n",
      "epoch : 60, t_cost : 0.000087, e_cost : 0.015699, elapsed time : 1675.92sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3898994707105885\n",
      "evalution_cost :  28.657021893926867\n",
      "epoch : 61, t_cost : 0.000087, e_cost : 0.014918, elapsed time : 1675.24sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3868277529918487\n",
      "evalution_cost :  29.621165879651016\n",
      "epoch : 62, t_cost : 0.000086, e_cost : 0.015420, elapsed time : 1670.40sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3849639268746614\n",
      "evalution_cost :  29.799013950498193\n",
      "epoch : 63, t_cost : 0.000086, e_cost : 0.015512, elapsed time : 1674.78sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning_cost :  0.3798909774304775\n",
      "evalution_cost :  29.11631612596102\n",
      "epoch : 64, t_cost : 0.000085, e_cost : 0.015157, elapsed time : 1674.53sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3829107412293524\n",
      "evalution_cost :  28.910586957375926\n",
      "epoch : 65, t_cost : 0.000085, e_cost : 0.015050, elapsed time : 1676.76sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3812884923290767\n",
      "evalution_cost :  29.828040192223853\n",
      "epoch : 66, t_cost : 0.000085, e_cost : 0.015527, elapsed time : 1677.71sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.38137124101285735\n",
      "evalution_cost :  31.17960940586636\n",
      "epoch : 67, t_cost : 0.000085, e_cost : 0.016231, elapsed time : 1674.30sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.37733651965027093\n",
      "evalution_cost :  30.1514961588\n",
      "epoch : 68, t_cost : 0.000084, e_cost : 0.015696, elapsed time : 1675.15sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3735245117859449\n",
      "evalution_cost :  30.963279956611586\n",
      "epoch : 69, t_cost : 0.000083, e_cost : 0.016118, elapsed time : 1677.08sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.37279019008838077\n",
      "evalution_cost :  28.97684192419547\n",
      "epoch : 70, t_cost : 0.000083, e_cost : 0.015084, elapsed time : 1682.28sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.37193684403564475\n",
      "evalution_cost :  25.617488831045193\n",
      "epoch : 71, t_cost : 0.000083, e_cost : 0.013335, elapsed time : 1778.23sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.36991485448288586\n",
      "evalution_cost :  37.54030612809402\n",
      "epoch : 72, t_cost : 0.000083, e_cost : 0.019542, elapsed time : 1571.49sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3658579328957785\n",
      "evalution_cost :  30.630722210915337\n",
      "epoch : 73, t_cost : 0.000082, e_cost : 0.015945, elapsed time : 1615.18sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3671397881771554\n",
      "evalution_cost :  30.655189570963557\n",
      "epoch : 74, t_cost : 0.000082, e_cost : 0.015958, elapsed time : 1576.62sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3659351177411736\n",
      "evalution_cost :  29.961365766823292\n",
      "epoch : 75, t_cost : 0.000082, e_cost : 0.015597, elapsed time : 1565.05sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.36485518422523455\n",
      "evalution_cost :  40.50898877317741\n",
      "epoch : 76, t_cost : 0.000081, e_cost : 0.021087, elapsed time : 1563.98sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3655313813196699\n",
      "evalution_cost :  29.525649157287262\n",
      "epoch : 77, t_cost : 0.000082, e_cost : 0.015370, elapsed time : 1567.96sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3643780231486744\n",
      "evalution_cost :  34.053787682914844\n",
      "epoch : 78, t_cost : 0.000081, e_cost : 0.017727, elapsed time : 1565.56sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.36651345712016337\n",
      "evalution_cost :  33.583115040346456\n",
      "epoch : 79, t_cost : 0.000082, e_cost : 0.017482, elapsed time : 1565.41sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3631980926666074\n",
      "evalution_cost :  29.872254890360637\n",
      "epoch : 80, t_cost : 0.000081, e_cost : 0.015550, elapsed time : 1566.89sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35771568565178313\n",
      "evalution_cost :  26.690680280578817\n",
      "epoch : 81, t_cost : 0.000080, e_cost : 0.013894, elapsed time : 1570.83sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35847486350348845\n",
      "evalution_cost :  29.152429587762526\n",
      "epoch : 82, t_cost : 0.000080, e_cost : 0.015176, elapsed time : 1562.95sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3585735911156007\n",
      "evalution_cost :  26.9378237302044\n",
      "epoch : 83, t_cost : 0.000080, e_cost : 0.014023, elapsed time : 1567.02sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35526198535444564\n",
      "evalution_cost :  27.059808638410686\n",
      "epoch : 84, t_cost : 0.000079, e_cost : 0.014086, elapsed time : 1568.43sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35562421490431007\n",
      "evalution_cost :  29.80191705766265\n",
      "epoch : 85, t_cost : 0.000079, e_cost : 0.015514, elapsed time : 1569.66sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.357438978790924\n",
      "evalution_cost :  31.858855179338207\n",
      "epoch : 86, t_cost : 0.000080, e_cost : 0.016585, elapsed time : 1567.41sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35638750480393355\n",
      "evalution_cost :  23.91221694140586\n",
      "epoch : 87, t_cost : 0.000079, e_cost : 0.012448, elapsed time : 1568.02sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35209777799263975\n",
      "evalution_cost :  30.695559065126872\n",
      "epoch : 88, t_cost : 0.000079, e_cost : 0.015979, elapsed time : 1567.15sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35292346552796516\n",
      "evalution_cost :  33.361369514126636\n",
      "epoch : 89, t_cost : 0.000079, e_cost : 0.017367, elapsed time : 1570.40sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35211294658301995\n",
      "evalution_cost :  49.82646568646305\n",
      "epoch : 90, t_cost : 0.000079, e_cost : 0.025938, elapsed time : 1566.50sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3545539656352048\n",
      "evalution_cost :  30.199678121534816\n",
      "epoch : 91, t_cost : 0.000079, e_cost : 0.015721, elapsed time : 1566.99sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35309628499908285\n",
      "evalution_cost :  30.68009652273031\n",
      "epoch : 92, t_cost : 0.000079, e_cost : 0.015971, elapsed time : 1567.35sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35963700967022305\n",
      "evalution_cost :  27.742733232837054\n",
      "epoch : 93, t_cost : 0.000080, e_cost : 0.014442, elapsed time : 1578.63sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3507339270972807\n",
      "evalution_cost :  29.57230023946977\n",
      "epoch : 94, t_cost : 0.000078, e_cost : 0.015394, elapsed time : 1567.26sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3509871603910142\n",
      "evalution_cost :  26.33745709528921\n",
      "epoch : 95, t_cost : 0.000078, e_cost : 0.013710, elapsed time : 1566.10sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3504912948801575\n",
      "evalution_cost :  31.026572415721603\n",
      "epoch : 96, t_cost : 0.000078, e_cost : 0.016151, elapsed time : 1568.01sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.3513609547253509\n",
      "evalution_cost :  30.345344155586645\n",
      "epoch : 97, t_cost : 0.000078, e_cost : 0.015797, elapsed time : 1568.58sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.34739895429856915\n",
      "evalution_cost :  33.11832017319466\n",
      "epoch : 98, t_cost : 0.000077, e_cost : 0.017240, elapsed time : 1568.94sec\n",
      "data1 :  (2295726, 16, 1) (2295726, 15, 16, 1) (2295726, 15, 16, 1) (2295726, 16, 1) (2295726, 1)\n",
      "data1 :  (983883, 16, 1) (983883, 15, 16, 1) (983883, 15, 16, 1) (983883, 16, 1) (983883, 1)\n",
      "traning_cost :  0.35002275786246173\n",
      "evalution_cost :  28.887036036769132\n",
      "epoch : 99, t_cost : 0.000078, e_cost : 0.015037, elapsed time : 1568.00sec\n",
      "\n",
      "#Best result at epoch 87\n",
      "t_cost : 0.000079, e_cost : 0.012448\n",
      "[<matplotlib.lines.Line2D object at 0x7fa50642e910>]\n",
      "0.02391141832842063\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de3Rd5X3m8e/v3HWx7rKxJGMb21zsmKtwyIUQEpKYtoPTJkydtintZMpKE6aZSduUTmeRljZrmstKO5mQDqyUTnohLqFdHSd1SyiQhqQ1WA7GYIOwMOC7JVvWzdKRztH5zR9nSxwLGR1jyTJ7P5+1tHT2Tefd3vJzXr3vu99t7o6IiIRXbL4LICIic0tBLyIScgp6EZGQU9CLiIScgl5EJOQS812AqZqamnzZsmXzXQwRkbeU7du3H3P35um2nXdBv2zZMjo6Oua7GCIibylm9urptqnpRkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQC03QD43m+eojL7Jjf998F0VE5LwSmqAfyxf42qN72LHvxHwXRUTkvFJW0JvZejPrNLMuM7tzmu2fNLNnzWyHmf3IzFaXbPvd4LhOM/vQbBa+VCZZPJVsvjBXbyEi8pY0Y9CbWRy4B7gZWA18rDTIAw+4+1p3vxL4EvDV4NjVwEZgDbAe+Ebw82ZdJlH8sdnc+Fz8eBGRt6xyavTrgC533+vuY8AmYEPpDu4+ULJYBUw8n3ADsMndR939ZaAr+HmzLhYzUvEY2Zxq9CIipcqZ1KwV2F+yfAB4+9SdzOzTwGeBFPC+kmO3Tjm2dZpjbwduB7jwwgvLKfe00omYavQiIlPMWmesu9/j7iuA3wH+xxkee5+7t7t7e3PztLNsliWdjDOaV9CLiJQqJ+gPAktKltuCdaezCfjwmzz2rGSSaroREZmqnKDfBqwys+VmlqLYubq5dAczW1Wy+NPAnuD1ZmCjmaXNbDmwCnjq7Is9vUwyrqYbEZEpZmyjd/e8md0BPAzEgfvdfZeZ3Q10uPtm4A4zuwnIASeA24Jjd5nZg8BuIA982t3nLImLNXoFvYhIqbKeMOXuW4AtU9bdVfL6M29w7BeAL7zZAp6JTCLOqMbRi4icIjR3xoKabkREphOyoFdnrIjIVKEK+nQyTlbDK0VEThGqoM8k4oyqRi8icopwBb1G3YiIvE7Igl6dsSIiU4Uq6NOJmKYpFhGZIlRBn0nGGS84uXGFvYjIhJAFffDwETXfiIhMClnQTzx8RDV6EZEJ4Qp6PWVKROR1QhX06aDpRnPSi4i8JlRBr6YbEZHXC2XQq0YvIvKacAV9YmLUjWr0IiITwhX0SXXGiohMFdKgV41eRGRCyIJeN0yJiEwVqqBPT4yjV2esiMikUAX9azV6Nd2IiEwIWdCrM1ZEZKpQBX06GF45qqAXEZkUqqA3M81JLyIyRaiCHvSUKRGRqUIY9HpurIhIqbKC3szWm1mnmXWZ2Z3TbP+sme02s51m9qiZLS3ZNm5mO4KvzbNZ+OlkknFG1XQjIjIpMdMOZhYH7gE+ABwAtpnZZnffXbLb00C7uw+b2a8DXwJ+Ptg24u5XznK5TyuTUNONiEipcmr064Aud9/r7mPAJmBD6Q7u/ri7DweLW4G22S1m+YpNN6rRi4hMKCfoW4H9JcsHgnWn8wngn0qWM2bWYWZbzezD0x1gZrcH+3T09PSUUaTTS6szVkTkFDM23ZwJM/sloB24oWT1Unc/aGYXAY+Z2bPu/lLpce5+H3AfQHt7u59NGTLJOP0jubP5ESIioVJOjf4gsKRkuS1Ydwozuwn4PeAWdx+dWO/uB4Pve4EfAFedRXlnlEnEdMOUiEiJcoJ+G7DKzJabWQrYCJwyesbMrgLupRjy3SXr680sHbxuAt4FlHbizjo13YiInGrGpht3z5vZHcDDQBy43913mdndQIe7bwa+DFQD3zEzgH3ufgtwGXCvmRUofqj88ZTROrMuk1BnrIhIqbLa6N19C7Blyrq7Sl7fdJrj/g1YezYFPFOZZFzTFIuIlNCdsSIiIRfCoI+TzRVwP6vBOyIioRHKoAc0DYKISCB0Qf/anPQKehERCGHQv1ajVzu9iAiEOOg1xFJEpCiEQR88IFw1ehERIIxBn9ADwkVESoUv6NV0IyJyitAFfXqi6UY1ehERIIRBr6YbEZFThS/oJztj1XQjIgKhDHrV6EVESoUu6Cfa6PXwERGRotAFvUbdiIicKnxBr85YEZFThC7ok3EjZpq9UkRkQuiC3syCOelVoxcRgRAGPehxgiIipcIZ9HpAuIjIpHAGvZpuREQmhTLoU6rRi4hMCmXQZ5JxPWFKRCQQ0qCPqelGRCQQ0qCPq+lGRCRQVtCb2Xoz6zSzLjO7c5rtnzWz3Wa208weNbOlJdtuM7M9wddts1n408kk1BkrIjJhxqA3szhwD3AzsBr4mJmtnrLb00C7u18OPAR8KTi2Afg88HZgHfB5M6ufveJPL5OMaRy9iEignBr9OqDL3fe6+xiwCdhQuoO7P+7uw8HiVqAteP0h4BF373X3E8AjwPrZKfrpqelGROQ15QR9K7C/ZPlAsO50PgH805kca2a3m1mHmXX09PSUUaQ3pnH0IiKvmdXOWDP7JaAd+PKZHOfu97l7u7u3Nzc3n3U50smYJjUTEQmUE/QHgSUly23BulOY2U3A7wG3uPvomRw72zKJOGP5AoWCz/VbiYic98oJ+m3AKjNbbmYpYCOwuXQHM7sKuJdiyHeXbHoY+KCZ1QedsB8M1s2piYePqFYvIgKJmXZw97yZ3UExoOPA/e6+y8zuBjrcfTPFpppq4DtmBrDP3W9x914z+0OKHxYAd7t775ycSYnJB4TnxqlIxef67UREzmszBj2Au28BtkxZd1fJ65ve4Nj7gfvfbAHfjPTEU6Y0xFJEJKx3xk7U6NV0IyIS0qDXc2NFRCaENOhfa6MXEYm6cAb9RBu9mm5ERMIZ9OmkOmNFRCaEMugnmm5G1XQjIhLWoFfTjYjIhJAHvWr0IiLhDPpE0HSjKRBEREIa9KrRi4hMCnXQD48p6EVEQhn08ZhRk0nQNzw230UREZl3oQx6gIaqFMdPKuhFREId9CdUoxcRCXPQpzk+pKAXEQlx0CdVoxcRIdRBn6b35Bjuem6siERbiIM+SW7cGRrNz3dRRETmVYiDPg1Ar0beiEjEhTjokwAaYikikRfioC/W6E8o6EUk4kIb9I1VKUA1ehGR0AZ9fRD0qtGLSNSFNuirUnFSiZg6Y0Uk8soKejNbb2adZtZlZndOs/09ZvYTM8ub2UenbBs3sx3B1+bZKngZZaahMqWgF5HIS8y0g5nFgXuADwAHgG1mttndd5fstg/4FeC3pvkRI+5+5SyU9Yw1VCnoRURmDHpgHdDl7nsBzGwTsAGYDHp3fyXYdl490qmxOkWvpkEQkYgrp+mmFdhfsnwgWFeujJl1mNlWM/vwdDuY2e3BPh09PT1n8KPfWL2abkREzkln7FJ3bwd+AfhTM1sxdQd3v8/d2929vbm5edbeWE03IiLlBf1BYEnJcluwrizufjD4vhf4AXDVGZTvrDRUpRjM5hnTQ8JFJMLKCfptwCozW25mKWAjUNboGTOrN7N08LoJeBclbftzrWFiLL3a6UUkwmYMenfPA3cADwPPAw+6+y4zu9vMbgEws2vN7ABwK3Cvme0KDr8M6DCzZ4DHgT+eMlpnTk0EvZpvRCTKyhl1g7tvAbZMWXdXyettFJt0ph73b8Dasyzjm6agFxEJ8Z2x8Np8Nwp6EYmyUAd9vYJeRCTcQV9XkcRMQS8i0RbqoE/EY9RWJBX0IhJpoQ56CG6a0vBKEYmw0Ad9Y1WK3iEFvYhEV+iDXvPdiEjUhT7oNYOliERd6IO+vjLFiZNjuPt8F0VEZF6EPugbqlLkC85ANj/fRRERmRehD/rGat00JSLRFvqgr69U0ItItIU+6Bur0oCCXkSiK/RBX1+VBOCEgl5EIir0QT9Roz+uoBeRiAp90Fek4mSSMXpPjs53UURE5kXogx6KtXrV6EUkqqIR9NWaBkFEoisSQd9QpaAXkeiKTNAf1wyWIhJRkQj6xqoUx9UZKyIRFY2gr06TzRUYHtN8NyISPZEI+obgIeFqvhGRKIpE0DdOBL06ZEUkgqIR9NUT892onV5EoqesoDez9WbWaWZdZnbnNNvfY2Y/MbO8mX10yrbbzGxP8HXbbBX8TDSq6UZEImzGoDezOHAPcDOwGviYma2ests+4FeAB6Yc2wB8Hng7sA74vJnVn32xz0yDmm5EJMLKqdGvA7rcfa+7jwGbgA2lO7j7K+6+EyhMOfZDwCPu3uvuJ4BHgPWzUO4zUpmKk07EdNOUiERSOUHfCuwvWT4QrCtHWcea2e1m1mFmHT09PWX+6PKZGU3VaTXdiEgknRedse5+n7u3u3t7c3PznLxHg26aEpGIKifoDwJLSpbbgnXlOJtjZ5XmuxGRqCon6LcBq8xsuZmlgI3A5jJ//sPAB82sPuiE/WCw7pxr1Hw3IhJRMwa9u+eBOygG9PPAg+6+y8zuNrNbAMzsWjM7ANwK3Gtmu4Jje4E/pPhhsQ24O1h3zmmqYhGJqkQ5O7n7FmDLlHV3lbzeRrFZZrpj7wfuP4syzoqGqjQjuXGGx/JUpso6bRGRUDgvOmPPBd00JSJRFZmgn7hpSs03IhI1kQn6xuqJu2M1xFJEoiU6QV9VnNhMTTciEjWRCfqGajXdiEg0RSboqzTfjYhEVGSC3sxorEpxTE03IhIxkQl6KDbf6OEjIhI10Qr6qrSabkQkciIV9E1quhGRCIpU0GsGSxGJomgFfXWKkdw4I2Pj810UEZFzJlJBPznfjTpkRSRCIhb0ujtWRKInUkGvu2NFJIoiFfSvNd0o6EUkOiIV9A2Tc9KrjV5EoiNSQV+dTpDSfDciEjGRCnozo6kqRY9q9CISIZEKeoDlzVXsOTo038UQETlnIhf0a1pq6TwySG68MN9FERE5JyIY9DWMjRfo6latXkSiIYJBXwvAcwf757kkIiLnRuSCfnlTFRXJOLsODcx3UUREzomygt7M1ptZp5l1mdmd02xPm9nfBtufNLNlwfplZjZiZjuCr/8zu8U/c/GYsbqlht0KehGJiBmD3sziwD3AzcBq4GNmtnrKbp8ATrj7SuBPgC+WbHvJ3a8Mvj45S+U+K2taath9eIBCwee7KCIic66cGv06oMvd97r7GLAJ2DBlnw3At4LXDwHvNzObvWLOrjUtNQyN5nm1d3i+iyIiMufKCfpWYH/J8oFg3bT7uHse6Acag23LzexpM/tXM7v+LMs7KyY6ZHcdUoesiITfXHfGHgYudPergM8CD5hZzdSdzOx2M+sws46enp45LhJcvGgBybipQ1ZEIqGcoD8ILClZbgvWTbuPmSWAWuC4u4+6+3EAd98OvARcPPUN3P0+d2939/bm5uYzP4szlErEWLVwgYZYikgklBP024BVZrbczFLARmDzlH02A7cFrz8KPObubmbNQWcuZnYRsArYOztFPztvay2OvHFXh6yIhNuMQR+0ud8BPAw8Dzzo7rvM7G4zuyXY7c+BRjProthEMzEE8z3ATjPbQbGT9pPu3jvbJ/FmrGmp5fjJMY4OaIIzEQm3RDk7ufsWYMuUdXeVvM4Ct05z3N8Bf3eWZZwTa1qKXQW7DvVzQW1mnksjIjJ3Indn7ITLFtdgBs8dVIesiIRbZIO+Kp3goqYqHuvs1kyWIhJqkQ16gE+9dyXP7O/jD7+3e76LIiIyZ8pqow+rj1zTRufRQe774V5WLVrAx69bOt9FEhGZdZGu0QP8zvpLufGSZn5/8y6e2DP3N2uJiJxrkQ/6eMz42seu4qKmKn75/qf47e88w5H+7HwXS0Rk1kS66WbCgkyShz75Tr7++B6+9W+v8t2dh/gPl7fQUldB84I0MTNeOX6SvT1DDGbzrGmp5YoltVyztJ62+sr5Lr6IyBuy8+3O0Pb2du/o6Ji399/fO8xXvt/JD1/s4cRwbnJ9KhFjeWMVFak4zx8eYDRfHKnzzhWNfPy6pXxg9SIS8cj/gSQi88TMtrt7+7TbFPSnlxsv0HtyjHzBWVyTIRazyfUvHh3kB509PPDkPg72jdC8IM0HVi/ipssW8s4VTWSS8XkuvYhEiYJ+Do0XnMdf6OY72/fzxJ5jDI+Nk0rEWNFczcqF1Vy8sJp3r2riira6yQ8KEZHZpqA/R0bz42zd28uPu46x5+ggXT1DHDgxgjssXJDmptWLuH5lE9cub6CpOj3fxRWRN/DEnh6+9M+d/OV/Wkd9VWq+izOjNwp6dcbOonQizg0XN3PDxa9Ntdw/nOPxzm6+v/sI//D0QR54ch8AKxdWc9WSOi5vq2VtWx0tdRlqMknSiRjn8cO5RCJhvOD8wXd309U9xN88+Sp3vG/VfBfprKhGfw6N5Qs8d6ifp17u5amXe3lmfx/HT46dsk8ybmSScVLxGKlEjLrKFG31FbTWVbC6pYb3XtLMwgWahE1kLj3YsZ/PPbSThQvSOPCj37mRdOL87ndTjf48kUrEuPrCeq6+sJ5P3rACd+dQf5ZnD/TTMzTKYDbHwEiebG6c3HiBsXyxM3jf8WF+3FVs/wdY21rLtcsaWFybYVFthobKFKlE8YOhsSrFkgYN+ZTzU+eRQb638xB3vG/leRuc2dw4f/rIi1zRVstvfvASfvn+p/juM4f56DVt8120N01BP4/MjNa6Ym19Ju7O84cHebyzm8de6ObbT+1jJDc+7b6XLa7hZy5fzI2XLKRpQYqaTFKjgGROPLzrCP0jOW69pm3GJsfugSy33f8URwayHBsa43/+3NpzVMoz89dbX+VQf5av3HoF71jRyKUXLOCbT+zlI1e3vmWbVRX0bxFmxuqWGla31PDpG1fi7gxk8xwdyBaHgI47ufECL/UMseXZw3z54U6+/HDn5PGpRIzaiiS1FUkaKlMsa6pkRXM1SxurSMSMcXfcoTIVpzqToCaT4MKGKlIJ3Rsg09t5oI87HvgJuXHniT3H+OJH1lKZmj5Ssrlxfu0vOxjI5vjZq1r59lP7WNtayy+8/UIABrI5Hn+hm3jMqEonqK9Msba1lvg5Hqk2kM1xz+NdXL+qiXeubALgE+9ezm8/tJMfdx3n3auaZvX9CgXHjDn/AFHQv0WZ2WRwl7rx0oX85+sv4lDfCNte6WUgm2dgJMdANsfASI6+4RzHh8Z47IUeHuw48IbvkUrEWNNSwxVtdYzmx3nl2DD7eocZGy+QisdIxo3KVGLyg6GtvpI1LTWsbaulrb4SA8yg4MX+ibF8gcFsjkP9WQ73jZDNjXPJBcUPr6nnATA0mqfzyAAXNlTRvGDuRynlxwvkCz5vf/24O6P5AqO5AqP5cbLBd7PitNpV6QTVqcSsDdPtH8mxt2eIfb3D1GSStNRVcEFthtHcOD1DoxwbGqOlNsOK5urXvedgNsd/+fbTNFWnubV9Cf/7sT28eGSQuzesoSpdjJVkPEZ9VZK6ihS//dBOdh7s595fuob3X7aI4yfH+Pzm51jSUMGzB/u591/30j+SO+U9Wusq+Pg7lvLz7UvOatTLYDbH0/v66Hj1BK11Gf5j+5JTgjWbG+dfnj/Kv+w+yuOdPfSP5Pjchy6d3H7LlS186eFO7ntiLxcvqmZwNE9uvMCqhQvK/iB6et8JBrJ5GipT1FYk2XWon4d3HeHRF7qpq0zy6zes5CPXtM5Zc5Y6YyOsfzjHvt5hHCdmhlnxl34wm6dvOMfuwwPs2NfHzoN9VKUSLG2sZGljFZlkfLIPYXgsP/lhsq93eLIf4Uy11Veworma5U1V1FUm2br3ONtfPUFuvPj7uagmzSUX1FAoOAPZHEPZPKlEjOogAAtBSOaCD6EFmQTV6QS5cadvZIy+4RzpRIyWoKksGY9x/OQYvSdH6R4c5XBflu7BLAUvvtfShioW1WZIxIr/Lu5M9qGM5MapTieorUhSmY4zPDrO4GiOkbFxljVVsaallhXNVbxy7CQ79vex+/AALXUVXLusgasvrAfg6ECWIwNZXjl2kq7uIV7qGTrlTuzTSSdirFpUzcWLFtBUneboQJbD/cW/6nLjBXLBHdu1lSkagpBNJ2Ik4zHMoGdwlCMDWY70Z183EOB0ajIJrrqwnhsvaWbDla3UVSb5zKYd/OOzh9l0+3Vcu6yBH77Yw29sepq+NziHz62/hE+9dyUAfcNj3PL1H7OvdxiA9126kE+9dwU1FUmGRvPs7x3m20/tY+veXtKJGB9YvYiPXN3G9auaGMzmeaLrGP/+0jGaqtO846JGrl5af8oHdFf3EN/ffYTv7zrKzgN9FEpi7uPXLeX3b1lDPGbs7Rni1//6J3QeHaS+MsmNly7kZ69q5fpVzaeU/euP7eEr33/xlHVN1WnWv20R7790Ed2DWZ450M+LRwa5vK2On7u6lTUtNTx7sJ8v/XMnP+o69rp/j7rKJO+/dBFdPUM8s7+PC2oy3P6ei/jVdy17UzV8jaOXs+LuZf3ijRecl48N8ezBfnoGR3EHB2IGqXiMdDJOVTrB4toMi2szpOIxdh8eYNehAV44MsjLx4Z4ueckJ8fGWb24husvbuLqC+vZ3zvMrkMDvHh0kHQiRk1Fkup0grF8gaHRPEOjeWJmxQ7peKz4l8NonqHRHKl4ceRSbUWSbG6cg30jHO7Lki8UaKhK0VCVonlBmsW1FbTUZkjGY7zaO8y+48P0DI0yXnDGgz+vazJJaioSZJJxhrJ5+kdyDI+NU5WOsyCTJBWP0dUzRM/ga88hXrggzZqWGg6cGGFP99Dr/s0aqlKsbK5mxcJqFi5Ik07GSCfipBMxMsk4mWSMgsPJ0TxD2TxHBrK8eHSQF48OcuJkjgtqM1xQm6GpOkUqHiMRj+EO/SNj9J4co28kF3wAOOPuNFWnuaAmzQW1GZY1VnFRczVLGysZzOY51DfC4f4RKlIJmqvTNFSl2Nc7zPZXT7DtlV66uodIxWNcvbSOrXt7+a0PXnzKsMPugSxP7+8L/pIzRvPjnBjOceLkGM0L0my89tSa9J5givCN65ZwzdKGaX+nOo8M8tdbi/NP9Q3nWJBJMDSaxx0WZBKcHM1T8OLvV01FkoIXmzAHs3kArmir5YaLm7l2eQNXLqnj6491ce8P9/JTay9g/dsW89///lmSceOPP3I5N1226LQ19OGxPA9u208yqFyMF5xHn+/m0ReOks0VP1wXZBKsXFjNroMDjI0XWNJQwf7eERqqUnz6xpVcuaSW3pPFf4+2hgrWLWsIrpfzo65jfP2xLipTcf7iV9fN+H9tOgp6ectwd0Zy46dt650NhYLjMGftv92DWV7qPsnSxkoW12Ymw61veIwd+/tIxmMsqsmwqCbNgszrm6zOV7sO9fOdjgP8w46DXN5Wx1/8yrXnrA19LF/gB53dPLL7KG31ldxwSTNrW2sZHsvT8coJtr58nMFsnrgZ8ZhxUXMVN122iJZpBjp884m9/NE/Pg/AFUvq+MYvXl3WgIjpDI/l2f7qCVrrKljWWEUsZvQNj/HdnYd5ZPdRrlxSx69dv7zs6zw8ln/Tv/sKehGZNeMFx+AtPaXHPz17mBeODPKpG1ect8M8z5TG0YvIrDnXI2Hmws1rF3Pz2sXzXYxzRmPnRERCTkEvIhJyCnoRkZArK+jNbL2ZdZpZl5ndOc32tJn9bbD9STNbVrLtd4P1nWb2odkruoiIlGPGoDezOHAPcDOwGviYma2estsngBPuvhL4E+CLwbGrgY3AGmA98I3g54mIyDlSTo1+HdDl7nvdfQzYBGyYss8G4FvB64eA91tx8PAGYJO7j7r7y0BX8PNEROQcKSfoW4H9JcsHgnXT7uPueaAfaCzzWMzsdjPrMLOOnp6e8ksvIiIzOi86Y939Pndvd/f25ubmmQ8QEZGylXPD1EFgSclyW7Buun0OmFkCqAWOl3nsKbZv337MzF4to1yn0wS8fgahcIviOUM0zzuK5wzRPO8zPeelp9tQTtBvA1aZ2XKKIb0R+IUp+2wGbgP+Hfgo8Ji7u5ltBh4ws68CLcAq4Kk3ejN3P6sqvZl1nO424LCK4jlDNM87iucM0Tzv2TznGYPe3fNmdgfwMBAH7nf3XWZ2N9Dh7puBPwf+ysy6gF6KHwYE+z0I7AbywKfd/c3NYysiIm9KWXPduPsWYMuUdXeVvM4Ct57m2C8AXziLMoqIyFk4LzpjZ9l9812AeRDFc4ZonncUzxmied6zds7n3TTFIiIyu8JYoxcRkRIKehGRkAtN0M808VpYmNkSM3vczHab2S4z+0ywvsHMHjGzPcH3+vku62wzs7iZPW1m3wuWlweT6HUFk+ql5ruMs83M6szsITN7wcyeN7N3hP1am9l/C363nzOzb5tZJozX2szuN7NuM3uuZN2019aKvhac/04zu/pM3isUQV/mxGthkQd+091XA9cBnw7O9U7gUXdfBTwaLIfNZ4DnS5a/CPxJMJneCYqT64XN/wL+2d0vBa6geP6hvdZm1gr8BtDu7m+jOKR7I+G81v+X4mSPpU53bW+meB/SKuB24M/O5I1CEfSUN/FaKLj7YXf/SfB6kOJ//FZOnVjuW8CH56eEc8PM2oCfBr4ZLBvwPoqT6EE4z7kWeA/F+1Rw9zF37yPk15risO+K4C77SuAwIbzW7v5DivcdlTrdtd0A/KUXbQXqzKzsZyGGJejLmjwtbIJ5/68CngQWufvhYNMRYNE8FWuu/CnwOaAQLDcCfcEkehDOa74c6AH+Imiy+qaZVRHia+3uB4GvAPsoBnw/sJ3wX+sJp7u2Z5VxYQn6yDGzauDvgP/q7gOl27w4ZjY042bN7GeAbnffPt9lOccSwNXAn7n7VcBJpjTThPBa11OsvS6nOG1KFa9v3oiE2by2YQn6M5487a3MzJIUQ/5v3P3vg9VHJ/6UC753z1f55sC7gFvM7BWKzXLvo9h2XRf8eQ/hvOYHgAPu/mSw/BDF4A/ztb4JeNnde9w9B/w9xesf9ms94XTX9qwyLixBPznxWnIcPyYAAAEUSURBVNAbv5HiRGuhE7RN/znwvLt/tWTTxMRyBN//37ku21xx99919zZ3X0bx2j7m7r8IPE5xEj0I2TkDuPsRYL+ZXRKsej/FeaNCe60pNtlcZ2aVwe/6xDmH+lqXON213Qz8cjD65jqgv6SJZ2buHoov4KeAF4GXgN+b7/LM4Xm+m+KfczuBHcHXT1Fss34U2AP8C9Aw32Wdo/N/L/C94PVFFGdD7QK+A6Tnu3xzcL5XAh3B9f4HoD7s1xr4A+AF4Dngr4B0GK818G2K/RA5in+9feJ01xYwiiMLXwKepTgqqez30hQIIiIhF5amGxEROQ0FvYhIyCnoRURCTkEvIhJyCnoRkZBT0IuIhJyCXkQk5P4/x+QKaDH05L0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "#parameter list\n",
    "sess=tf.compat.v1.Session()\n",
    "name='lstm'\n",
    "timesize=16\n",
    "timesize_for_calc_correlation=70\n",
    "positive_correlation_stock_num=15\n",
    "negative_correlation_sotck_num=15\n",
    "train_test_rate=0.7\n",
    "batch_size=512\n",
    "#\n",
    "kospi=StockData(\n",
    "    'NASDAQ/sample',\n",
    "    'NASDAQ/NDX.csv',\n",
    "    timesize,timesize_for_calc_correlation,\n",
    "    positive_correlation_stock_num,\n",
    "    negative_correlation_sotck_num,\n",
    "    train_test_rate,\n",
    "    batch_size)\n",
    "\n",
    "print('\\n#training#')\n",
    "lstmModel=LSTM_Model(\n",
    "    sess,\n",
    "    name,\n",
    "    timesize,\n",
    "    positive_correlation_stock_num,\n",
    "    negative_correlation_sotck_num\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "result_dic={}\n",
    "evalution_costplt=[]\n",
    "for i in range(100):\n",
    "    #epoch start\n",
    "    start_time = time.time()\n",
    "    training_cost=0\n",
    "    evalution_cost=0\n",
    "\n",
    "    #training batch\n",
    "    for batch in kospi.getBatch('training'):\n",
    "        c,_=lstmModel.training(batch['y'],batch['xp'],batch['xn'],batch['xi'],batch['target'])\n",
    "        training_cost+=c\n",
    "       \n",
    "    #evaluation batch\n",
    "    for batch in kospi.getBatch('evaluation'):\n",
    "        c=lstmModel.returnCost(batch['y'],batch['xp'],batch['xn'],batch['xi'],batch['target'])\n",
    "        evalution_cost+=c\n",
    "\n",
    "    print(\"traning_cost : \" , training_cost)\n",
    "    print(\"evalution_cost : \" , evalution_cost)\n",
    "   \n",
    "    #epoch end\n",
    "    elapsed_time = time.time()-start_time\n",
    "    training_cost=training_cost/kospi.batchNum['training']\n",
    "    evalution_cost=evalution_cost/kospi.batchNum['evaluation']\n",
    "    result_dic[i]=[training_cost,evalution_cost]\n",
    "    evalution_costplt.append(evalution_cost)\n",
    "\n",
    "    print('epoch : {}, t_cost : {:0.6f}, e_cost : {:0.6f}, elapsed time : {:0.2f}sec'.format(\n",
    "        i,training_cost,evalution_cost,elapsed_time))\n",
    "#\n",
    "\n",
    "sorted_result=sorted(result_dic,key=lambda k:result_dic[k][1])\n",
    "bestEpoch=sorted_result[0]\n",
    "print('\\n#Best result at epoch {}'.format(bestEpoch))\n",
    "print('t_cost : {:0.6f}, e_cost : {:0.6f}'.format(result_dic[bestEpoch][0],result_dic[bestEpoch][1]))\n",
    "print(plt.plot(evalution_costplt))\n",
    "print(np.mean(evalution_costplt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
