{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "from configs.config import *\n",
    "from preprocess.get_data import *\n",
    "from models.DA_RNN import *\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DARNN():\n",
    "    def __init__(self, config, scope = 'DA_RNN'):\n",
    "        # placeholder\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        self.input_keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "        self.output_keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "\n",
    "        self.X = tf.compat.v1.placeholder(tf.float32, shape = [None, config.n, config.timestep, config.input_size])\n",
    "        self.Y_prev = tf.compat.v1.placeholder(tf.float32, shape = [None, config.timestep - 1, 1])\n",
    "        self.Y = tf.compat.v1.placeholder(tf.float32, shape = [None, 1])\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        # <batch_size, n , timestep * input_size>\n",
    "        x_flat = tf.reshape(self.X, shape = [-1, config.n, config.timestep * config.input_size])\n",
    "        # <batch_size, timestep, n * input_size>\n",
    "        x_t = tf.reshape(tf.transpose(self.X, perm = [0, 2, 1, 3]), shape = [-1, config.timestep, config.n * config.input_size])\n",
    "        \n",
    "        with tf.compat.v1.variable_scope(scope + '_Encoder'):\n",
    "\n",
    "            encoder_lstms = [tf.compat.v1.nn.rnn_cell.BasicLSTMCell(hid_size, forget_bias = 0.0) for hid_size in config.hidden_sizes]\n",
    "            if config.input_keep_prob < 1.0 or config.output_keep_prob < 1.0:\n",
    "                encoder_lstms = [tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, input_keep_prob = self.input_keep_prob,\n",
    "                                                                  output_keep_prob = self.output_keep_prob) for lstm in encoder_lstms]\n",
    "            encoder_cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell(encoder_lstms, state_is_tuple = True)\n",
    "\n",
    "            # batch_size * last_hidden_size\n",
    "            encoder_s_state = encoder_cells.zero_state(config.batch_size, tf.float32)\n",
    "            \n",
    "            last_hidden_size = config.hidden_sizes[-1]\n",
    "\n",
    "            # timestep * <batch_size, last_hidden_size>\n",
    "            X_encodeds = []\n",
    "\n",
    "            # attention layer\n",
    "            en_w_e = tf.Variable(tf.random.truncated_normal(shape=[2 * last_hidden_size, config.timestep]), dtype=tf.float32)\n",
    "            en_u_e = tf.Variable(tf.random.truncated_normal(shape=[config.timestep * config.input_size, config.timestep]),\n",
    "                                 dtype=tf.float32)\n",
    "            en_v_e = tf.Variable(tf.random.truncated_normal(shape=[config.timestep, 1]))\n",
    "\n",
    "            # LSTM run by step\n",
    "            for t in range(config.timestep):\n",
    "\n",
    "                # batch_size * 2m\n",
    "                h_s_concat = tf.concat([encoder_s_state[-1].h, encoder_s_state[-1].c], axis = 1)\n",
    "\n",
    "                hs_part = tf.matmul(h_s_concat, en_w_e)\n",
    "\n",
    "                # n * <batch_size, 1>\n",
    "                e_ks = []\n",
    "                for i in range(config.n):\n",
    "                    e_k = hs_part + tf.matmul(x_flat[:, i, :], en_u_e)\n",
    "                    e_k = tf.matmul( tf.nn.tanh(e_k), en_v_e)\n",
    "\n",
    "                    e_ks.append(e_k)\n",
    "\n",
    "                e_ks = tf.transpose(e_ks, perm = [1, 0, 2])\n",
    "                e_ks = tf.reshape(e_ks, shape = [-1, config.n])\n",
    "                # <batch_size, n>\n",
    "                alpha_k = tf.nn.softmax(e_ks)\n",
    "                #print('a_k_diag', tf.matrix_diag(alpha_k))\n",
    "\n",
    "                # <batch_size, 1, n> = <batch_size, 1, n> matmul <batch_size, n, n>\n",
    "                \n",
    "                # x_tilde = tf.matmul( tf.reshape(tf.matmul(x_t[:, t, :], en_feature_e), shape = (-1, 1, config.n)) , tf.matrix_diag(alpha_k))\n",
    "                # one feature\n",
    "                x_tilde = tf.matmul( tf.reshape(x_t[:, t, :], shape = (-1, 1, config.n)) , tf.compat.v1.matrix_diag(alpha_k))\n",
    "                # <batch_size, n>\n",
    "                x_tilde = tf.reshape(x_tilde, shape = [-1, config.n])\n",
    "\n",
    "                \n",
    "                #print('State size:', encoder_cells.state_size)\n",
    "                encoder_h_state, encoder_s_state = encoder_cells.call(x_tilde, encoder_s_state)\n",
    "\n",
    "                X_encodeds.append(encoder_h_state)\n",
    "    \n",
    "        with tf.compat.v1.variable_scope(scope + '_Decoder'):\n",
    "            decoder_lstms = [tf.compat.v1.nn.rnn_cell.BasicLSTMCell(hid_size) for hid_size in config.hidden_sizes]\n",
    "            if config.input_keep_prob < 1.0 or config.output_keep_prob < 1.0:\n",
    "                decoder_lstms = [tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, input_keep_prob = self.input_keep_prob,\n",
    "                                                                  output_keep_prob = self.output_keep_prob) for lstm in decoder_lstms]\n",
    "            decoder_cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell(decoder_lstms, state_is_tuple = True)\n",
    "\n",
    "            decoder_s_state = decoder_cells.zero_state(config.batch_size, tf.float32)\n",
    "            \n",
    "            last_hidden_size = config.hidden_sizes[-1]\n",
    "\n",
    "            # attention layer\n",
    "            de_w_e = tf.Variable(tf.random.truncated_normal(shape=[2 * last_hidden_size, last_hidden_size]), dtype=tf.float32)\n",
    "            de_u_e = tf.Variable(tf.random.truncated_normal(shape=[last_hidden_size, last_hidden_size]), dtype=tf.float32)\n",
    "            de_v_e = tf.Variable(tf.random.truncated_normal(shape=[last_hidden_size, 1]))\n",
    "\n",
    "            de_w_tilde = tf.Variable(tf.random.truncated_normal(shape = [last_hidden_size + 1, 1]), dtype = tf.float32)\n",
    "            de_b_tilde = tf.Variable(tf.constant(0.0, shape = [1], dtype = tf.float32))\n",
    "\n",
    "            # lstm run by step\n",
    "            for t in range(config.timestep):\n",
    "                \n",
    "                # batch_size * 2m\n",
    "                d_s_concat = tf.concat([decoder_s_state[-1].h, decoder_s_state[-1].c], axis = 1)\n",
    "                ds_part = tf.matmul(d_s_concat, de_w_e)\n",
    "\n",
    "                # timestep * <batch_size, 1>\n",
    "                l_ks = []\n",
    "                for i in range(config.timestep):\n",
    "                    l_k = ds_part + tf.matmul(X_encodeds[t], de_u_e)\n",
    "                    l_k = tf.matmul(tf.nn.tanh(l_k), de_v_e)\n",
    "\n",
    "                    l_ks.append(l_k)\n",
    "\n",
    "                l_ks = tf.transpose(l_ks, perm = [1, 0, 2])\n",
    "                l_ks = tf.reshape(l_ks, shape = [-1, config.timestep])\n",
    "                # <batch_size, timestep>\n",
    "                beta_k = tf.nn.softmax(l_ks)\n",
    "                \n",
    "                # <batch_size, m>\n",
    "                c_t = None\n",
    "                for i in range(config.timestep):\n",
    "                    if c_t is None:\n",
    "                        c_t = X_encodeds[i] * tf.reshape(beta_k[:, i], shape = [-1, 1])\n",
    "                    else:\n",
    "                        c_t += X_encodeds[i] * tf.reshape(beta_k[:, i], shape = [-1, 1])\n",
    "\n",
    "                # decoder input\n",
    "                # concat y_t and c_t, shape is (batch_size, m + 1)\n",
    "                y_tilde = tf.concat([ tf.reshape(self.Y_prev[:, max(0, t - 1)], shape = [-1, 1]), c_t], axis = 1)\n",
    "                y_tilde = tf.matmul(y_tilde, de_w_tilde) + de_b_tilde\n",
    "\n",
    "                # run lstm cell\n",
    "                de_h_state, decoder_s_state = decoder_cells.call(y_tilde, decoder_s_state)\n",
    "\n",
    "            infer_w_y = tf.Variable(tf.random.truncated_normal(shape = [last_hidden_size + last_hidden_size, last_hidden_size]), dtype = tf.float32)\n",
    "            infer_b_w = tf.Variable(tf.constant(0.0, shape = [last_hidden_size], dtype = tf.float32))\n",
    "\n",
    "            infer_v_y = tf.Variable(tf.random.truncated_normal(shape = [last_hidden_size, 1]), dtype = tf.float32)\n",
    "            infer_b_v = tf.Variable(tf.constant(0.0, shape = [1], dtype = tf.float32))\n",
    "\n",
    "            # compute predicted value\n",
    "            infer_concat = tf.concat([decoder_s_state[-1].h, X_encodeds[-1]], axis = 1)\n",
    "            y_pred = tf.matmul(infer_concat, infer_w_y) + infer_b_w\n",
    "            y_pred = tf.matmul(y_pred, infer_v_y) + infer_b_v\n",
    "\n",
    "        # assignment\n",
    "        self.loss = tf.losses.mean_squared_error(self.Y, y_pred)\n",
    "        self.train_op = tf.compat.v1.train.AdamOptimizer(config.lr).minimize(self.loss)\n",
    "        \n",
    "        self.Y_pred = y_pred\n",
    "\n",
    "    def train(self, batch_data, y_scaler, sess):\n",
    "        all_loss = []\n",
    "        all_perc_loss = []\n",
    "        all_rmse = []\n",
    "        for ds in batch_data:\n",
    "            _, ls, pred = sess.run([self.train_op, self.loss, self.Y_pred], feed_dict = {self.X : ds[0], self.Y_prev : ds[1], self.Y : ds[2],\n",
    "                                                              self.input_keep_prob : self.config.input_keep_prob,\n",
    "                                                              self.output_keep_prob : self.config.output_keep_prob})\n",
    "            \n",
    "            y_pre_list = []\n",
    "            y_real_list = []\n",
    "            for j in range(len(ds[2])):\n",
    "                if self.config.is_scaled:\n",
    "                    y_pre_list.append(y_scaler.inverse_transform([ pred[j] ]))\n",
    "                    y_real_list.append(y_scaler.inverse_transform([ ds[2][j] ]))\n",
    "\n",
    "            loss = np.mean( np.divide(abs(np.subtract(y_pre_list, y_real_list)), y_real_list))\n",
    "            rmse = np.sqrt(np.mean(np.subtract(y_pre_list, y_real_list) ** 2))\n",
    "        \n",
    "            all_perc_loss.append(loss)\n",
    "            all_rmse.append(rmse)\n",
    "            all_loss.append(ls)\n",
    "        return np.mean(all_loss), np.mean(all_perc_loss), np.mean(all_rmse)\n",
    "\n",
    "    def predict(self, batch_data, y_scaler, sess):\n",
    "\n",
    "        all_loss = []\n",
    "        all_perc_loss = []\n",
    "        all_rmse = []\n",
    "        for ds in batch_data:\n",
    "            ls, pred = sess.run([self.loss, self.Y_pred], feed_dict = {self.X : ds[0], self.Y_prev : ds[1], self.Y : ds[2],\n",
    "                                                        self.input_keep_prob : 1.0,\n",
    "                                                        self.output_keep_prob : 1.0})\n",
    "            if self.config.is_scaled:\n",
    "                y_pre_list = []\n",
    "                y_real_list = []\n",
    "                for j in range(len(ds[2])):\n",
    "                    y_pre_list.append(y_scaler.inverse_transform([ pred[j] ]))\n",
    "                    y_real_list.append(y_scaler.inverse_transform([ ds[2][j] ]))\n",
    "\n",
    "                loss = np.mean( np.divide(abs(np.subtract(y_pre_list, y_real_list)), y_real_list))\n",
    "                rmse = np.sqrt(np.mean(np.subtract(y_pre_list, y_real_list) ** 2))\n",
    "            \n",
    "                all_perc_loss.append(loss)\n",
    "                all_rmse.append(rmse)\n",
    "            all_loss.append(ls)\n",
    "\n",
    "        return np.mean(all_loss), np.mean(all_perc_loss), np.mean(all_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-80aa3c881466>:21: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-2-80aa3c881466>:25: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5ec6ffdf3ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trainable parameter count:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_num_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mlast_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mbest_valid_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5ec6ffdf3ba3>\u001b[0m in \u001b[0;36mget_num_params\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mnum_params\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnum_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5ec6ffdf3ba3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mnum_params\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnum_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "MODEL_PATH = './checkpoints/taxi5_da_rnn.ckpt'\n",
    "is_train = True\n",
    "RMSE_STOP_THRESHOLD = 0.00003\n",
    "\n",
    "# --------------------- Data Process -----------------------\n",
    "# TaxiNYConfig, NasdaqConfig\n",
    "config = TaxiNYConfig\n",
    "# TaxiNYDataset, NasdaqDataset\n",
    "ds_handler = TaxiNYDataset(config)\n",
    "\n",
    "dataset = ds_handler.get_dataset()\n",
    "y_scaler = dataset[-1, -1, 0]\n",
    "\n",
    "train_ds, valid_ds, test_ds = ds_handler.divide_three_ds(dataset)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "model = DARNN(config)\n",
    "sess = tf.compat.v1.Session()\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "def get_num_params():\n",
    "    num_params = 0\n",
    "    for variable in tf.compat.v1.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        num_params += reduce(mul, [dim.value for dim in shape], 1)\n",
    "    return num_params\n",
    "\n",
    "if is_train:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    print('Trainable parameter count:', get_num_params())\n",
    "    last_rmse = 100.0\n",
    "    best_valid_rmse = 100.0\n",
    "    for i in range(1000):\n",
    "\n",
    "        batch_data = ds_handler.get_batch_data(train_ds)\n",
    "        loss, mape, rmse = model.train(batch_data, y_scaler, sess)\n",
    "        print('Epoch', i, 'Train Loss:', loss, 'MAPE(%):', mape * 100, 'RMSE:', rmse)\n",
    "        if abs(last_rmse - rmse) < RMSE_STOP_THRESHOLD:\n",
    "            break\n",
    "        last_rmse = rmse\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            batch_data = ds_handler.get_batch_data(valid_ds)\n",
    "            loss, mape, rmse = model.predict(batch_data, y_scaler, sess)\n",
    "            print('Valid Loss:', loss, 'MAPE(%):', mape * 100, 'RMSE:', rmse)\n",
    "            if best_valid_rmse > rmse:\n",
    "                best_valid_rmse = rmse\n",
    "                # save model\n",
    "                saver.save(sess, MODEL_PATH)\n",
    "\n",
    "    print('Training completed')\n",
    "    print('Best valid rmse:', best_valid_rmse)\n",
    "\n",
    "else:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "\n",
    "    batch_data = ds_handler.get_batch_data(valid_ds)\n",
    "    loss, mape, rmse = model.predict(batch_data, y_scaler, sess)\n",
    "    print('Valid Loss:', loss, 'MAPE(%):', mape * 100, 'RMSE:', rmse)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
