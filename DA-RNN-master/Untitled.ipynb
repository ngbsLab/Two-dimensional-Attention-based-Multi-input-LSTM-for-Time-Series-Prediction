{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DARNN():\n",
    "    def __init__(self, config, scope = 'DA_RNN'):\n",
    "        # placeholder\n",
    "        self.input_keep_prob = tf.placeholder(tf.float32)\n",
    "        self.output_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        self.X = tf.placeholder(tf.float32, shape = [None, config.n, config.timestep, config.input_size])\n",
    "        self.Y_prev = tf.placeholder(tf.float32, shape = [None, config.timestep - 1, 1])\n",
    "        self.Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        # <batch_size, n , timestep * input_size>\n",
    "        x_flat = tf.reshape(self.X, shape = [-1, config.n, config.timestep * config.input_size])\n",
    "        # <batch_size, timestep, n * input_size>\n",
    "        x_t = tf.reshape(tf.transpose(self.X, perm = [0, 2, 1, 3]), shape = [-1, config.timestep, config.n * config.input_size])\n",
    "        \n",
    "        with tf.variable_scope(scope + '_Encoder'):\n",
    "\n",
    "            encoder_lstms = [tf.contrib.rnn.BasicLSTMCell(hid_size, forget_bias = 0.0) for hid_size in config.hidden_sizes]\n",
    "            if config.input_keep_prob < 1.0 or config.output_keep_prob < 1.0:\n",
    "                encoder_lstms = [tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = self.input_keep_prob,\n",
    "                                                                  output_keep_prob = self.output_keep_prob) for lstm in encoder_lstms]\n",
    "            encoder_cells = tf.contrib.rnn.MultiRNNCell(encoder_lstms, state_is_tuple = True)\n",
    "\n",
    "            # batch_size * last_hidden_size\n",
    "            encoder_s_state = encoder_cells.zero_state(config.batch_size, tf.float32)\n",
    "            \n",
    "            last_hidden_size = config.hidden_sizes[-1]\n",
    "\n",
    "            # timestep * <batch_size, last_hidden_size>\n",
    "            X_encodeds = []\n",
    "\n",
    "            # attention layer\n",
    "            en_w_e = tf.Variable(tf.truncated_normal(shape=[2 * last_hidden_size, config.timestep]), dtype=tf.float32)\n",
    "            en_u_e = tf.Variable(tf.truncated_normal(shape=[config.timestep * config.input_size, config.timestep]),\n",
    "                                 dtype=tf.float32)\n",
    "            en_v_e = tf.Variable(tf.truncated_normal(shape=[config.timestep, 1]))\n",
    "\n",
    "            # LSTM run by step\n",
    "            for t in range(config.timestep):\n",
    "\n",
    "                # batch_size * 2m\n",
    "                h_s_concat = tf.concat([encoder_s_state[-1].h, encoder_s_state[-1].c], axis = 1)\n",
    "\n",
    "                hs_part = tf.matmul(h_s_concat, en_w_e)\n",
    "\n",
    "                # n * <batch_size, 1>\n",
    "                e_ks = []\n",
    "                for i in range(config.n):\n",
    "                    e_k = hs_part + tf.matmul(x_flat[:, i, :], en_u_e)\n",
    "                    e_k = tf.matmul( tf.nn.tanh(e_k), en_v_e)\n",
    "\n",
    "                    e_ks.append(e_k)\n",
    "\n",
    "                e_ks = tf.transpose(e_ks, perm = [1, 0, 2])\n",
    "                e_ks = tf.reshape(e_ks, shape = [-1, config.n])\n",
    "                # <batch_size, n>\n",
    "                alpha_k = tf.nn.softmax(e_ks)\n",
    "                #print('a_k_diag', tf.matrix_diag(alpha_k))\n",
    "\n",
    "                # <batch_size, 1, n> = <batch_size, 1, n> matmul <batch_size, n, n>\n",
    "                \n",
    "                # x_tilde = tf.matmul( tf.reshape(tf.matmul(x_t[:, t, :], en_feature_e), shape = (-1, 1, config.n)) , tf.matrix_diag(alpha_k))\n",
    "                # one feature\n",
    "                x_tilde = tf.matmul( tf.reshape(x_t[:, t, :], shape = (-1, 1, config.n)) , tf.matrix_diag(alpha_k))\n",
    "                # <batch_size, n>\n",
    "                x_tilde = tf.reshape(x_tilde, shape = [-1, config.n])\n",
    "\n",
    "                \n",
    "                #print('State size:', encoder_cells.state_size)\n",
    "                encoder_h_state, encoder_s_state = encoder_cells.call(x_tilde, encoder_s_state)\n",
    "\n",
    "                X_encodeds.append(encoder_h_state)\n",
    "    \n",
    "        with tf.variable_scope(scope + '_Decoder'):\n",
    "            decoder_lstms = [tf.contrib.rnn.BasicLSTMCell(hid_size) for hid_size in config.hidden_sizes]\n",
    "            if config.input_keep_prob < 1.0 or config.output_keep_prob < 1.0:\n",
    "                decoder_lstms = [tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = self.input_keep_prob,\n",
    "                                                                  output_keep_prob = self.output_keep_prob) for lstm in decoder_lstms]\n",
    "            decoder_cells = tf.contrib.rnn.MultiRNNCell(decoder_lstms, state_is_tuple = True)\n",
    "\n",
    "            decoder_s_state = decoder_cells.zero_state(config.batch_size, tf.float32)\n",
    "            \n",
    "            last_hidden_size = config.hidden_sizes[-1]\n",
    "\n",
    "            # attention layer\n",
    "            de_w_e = tf.Variable(tf.truncated_normal(shape=[2 * last_hidden_size, last_hidden_size]), dtype=tf.float32)\n",
    "            de_u_e = tf.Variable(tf.truncated_normal(shape=[last_hidden_size, last_hidden_size]), dtype=tf.float32)\n",
    "            de_v_e = tf.Variable(tf.truncated_normal(shape=[last_hidden_size, 1]))\n",
    "\n",
    "            de_w_tilde = tf.Variable(tf.truncated_normal(shape = [last_hidden_size + 1, 1]), dtype = tf.float32)\n",
    "            de_b_tilde = tf.Variable(tf.constant(0.0, shape = [1], dtype = tf.float32))\n",
    "\n",
    "            # lstm run by step\n",
    "            for t in range(config.timestep):\n",
    "                \n",
    "                # batch_size * 2m\n",
    "                d_s_concat = tf.concat([decoder_s_state[-1].h, decoder_s_state[-1].c], axis = 1)\n",
    "                ds_part = tf.matmul(d_s_concat, de_w_e)\n",
    "\n",
    "                # timestep * <batch_size, 1>\n",
    "                l_ks = []\n",
    "                for i in range(config.timestep):\n",
    "                    l_k = ds_part + tf.matmul(X_encodeds[t], de_u_e)\n",
    "                    l_k = tf.matmul(tf.nn.tanh(l_k), de_v_e)\n",
    "\n",
    "                    l_ks.append(l_k)\n",
    "\n",
    "                l_ks = tf.transpose(l_ks, perm = [1, 0, 2])\n",
    "                l_ks = tf.reshape(l_ks, shape = [-1, config.timestep])\n",
    "                # <batch_size, timestep>\n",
    "                beta_k = tf.nn.softmax(l_ks)\n",
    "                \n",
    "                # <batch_size, m>\n",
    "                c_t = None\n",
    "                for i in range(config.timestep):\n",
    "                    if c_t is None:\n",
    "                        c_t = X_encodeds[i] * tf.reshape(beta_k[:, i], shape = [-1, 1])\n",
    "                    else:\n",
    "                        c_t += X_encodeds[i] * tf.reshape(beta_k[:, i], shape = [-1, 1])\n",
    "\n",
    "                # decoder input\n",
    "                # concat y_t and c_t, shape is (batch_size, m + 1)\n",
    "                y_tilde = tf.concat([ tf.reshape(self.Y_prev[:, max(0, t - 1)], shape = [-1, 1]), c_t], axis = 1)\n",
    "                y_tilde = tf.matmul(y_tilde, de_w_tilde) + de_b_tilde\n",
    "\n",
    "                # run lstm cell\n",
    "                de_h_state, decoder_s_state = decoder_cells.call(y_tilde, decoder_s_state)\n",
    "\n",
    "            infer_w_y = tf.Variable(tf.truncated_normal(shape = [last_hidden_size + last_hidden_size, last_hidden_size]), dtype = tf.float32)\n",
    "            infer_b_w = tf.Variable(tf.constant(0.0, shape = [last_hidden_size], dtype = tf.float32))\n",
    "\n",
    "            infer_v_y = tf.Variable(tf.truncated_normal(shape = [last_hidden_size, 1]), dtype = tf.float32)\n",
    "            infer_b_v = tf.Variable(tf.constant(0.0, shape = [1], dtype = tf.float32))\n",
    "\n",
    "            # compute predicted value\n",
    "            infer_concat = tf.concat([decoder_s_state[-1].h, X_encodeds[-1]], axis = 1)\n",
    "            y_pred = tf.matmul(infer_concat, infer_w_y) + infer_b_w\n",
    "            y_pred = tf.matmul(y_pred, infer_v_y) + infer_b_v\n",
    "\n",
    "        # assignment\n",
    "        self.loss = tf.losses.mean_squared_error(self.Y, y_pred)\n",
    "        self.train_op = tf.train.AdamOptimizer(config.lr).minimize(self.loss)\n",
    "        \n",
    "        self.Y_pred = y_pred\n",
    "\n",
    "    def train(self, batch_data, y_scaler, sess):\n",
    "        all_loss = []\n",
    "        all_perc_loss = []\n",
    "        all_rmse = []\n",
    "        for ds in batch_data:\n",
    "            _, ls, pred = sess.run([self.train_op, self.loss, self.Y_pred], feed_dict = {self.X : ds[0], self.Y_prev : ds[1], self.Y : ds[2],\n",
    "                                                              self.input_keep_prob : self.config.input_keep_prob,\n",
    "                                                              self.output_keep_prob : self.config.output_keep_prob})\n",
    "            \n",
    "            y_pre_list = []\n",
    "            y_real_list = []\n",
    "            for j in range(len(ds[2])):\n",
    "                if self.config.is_scaled:\n",
    "                    y_pre_list.append(y_scaler.inverse_transform([ pred[j] ]))\n",
    "                    y_real_list.append(y_scaler.inverse_transform([ ds[2][j] ]))\n",
    "\n",
    "            loss = np.mean( np.divide(abs(np.subtract(y_pre_list, y_real_list)), y_real_list))\n",
    "            rmse = np.sqrt(np.mean(np.subtract(y_pre_list, y_real_list) ** 2))\n",
    "        \n",
    "            all_perc_loss.append(loss)\n",
    "            all_rmse.append(rmse)\n",
    "            all_loss.append(ls)\n",
    "        return np.mean(all_loss), np.mean(all_perc_loss), np.mean(all_rmse)\n",
    "\n",
    "    def predict(self, batch_data, y_scaler, sess):\n",
    "\n",
    "        all_loss = []\n",
    "        all_perc_loss = []\n",
    "        all_rmse = []\n",
    "        for ds in batch_data:\n",
    "            ls, pred = sess.run([self.loss, self.Y_pred], feed_dict = {self.X : ds[0], self.Y_prev : ds[1], self.Y : ds[2],\n",
    "                                                        self.input_keep_prob : 1.0,\n",
    "                                                        self.output_keep_prob : 1.0})\n",
    "            if self.config.is_scaled:\n",
    "                y_pre_list = []\n",
    "                y_real_list = []\n",
    "                for j in range(len(ds[2])):\n",
    "                    y_pre_list.append(y_scaler.inverse_transform([ pred[j] ]))\n",
    "                    y_real_list.append(y_scaler.inverse_transform([ ds[2][j] ]))\n",
    "\n",
    "                loss = np.mean( np.divide(abs(np.subtract(y_pre_list, y_real_list)), y_real_list))\n",
    "                rmse = np.sqrt(np.mean(np.subtract(y_pre_list, y_real_list) ** 2))\n",
    "            \n",
    "                all_perc_loss.append(loss)\n",
    "                all_rmse.append(rmse)\n",
    "            all_loss.append(ls)\n",
    "\n",
    "        return np.mean(all_loss), np.mean(all_perc_loss), np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
